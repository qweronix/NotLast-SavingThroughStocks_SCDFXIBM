{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, 
{"metadata": {}, "cell_type": "markdown", 
"source": "# PREDICTING THE STOCK MARKET WITH WATSON\n\n## Part I: Introduction\n
\n
In this Jupyter Notebook you will learn step-by-step how to extract financial data from one of the most popular public databases for econometric data, as well as the cleansing and preparation processes of the data science traditional workflow.\n\nAfter preparing the data into an adequate format, structured into a pandas dataframe, we train a predictive model utilizing techniques such as curve-fitting and ARIMA (AutoRegressive Integrated Moving Average).\n\nLastly, we create interactive plots to visualize the collected data, as well as the results of the forecasters trained.\n\n## Table of Contents\n\n#### 1. Mining Stock Market Data\n* 1.1. Quandl API Setup\n* 1.2. Downloading the WIKI/PRICES Table\n* 1.3. Visualizing the Collected Data\n\n#### 2. Training a Predictive Model using Open-Source Libraries\n* 2.1. Fbprophet Setup\n* 2.2. Data Preparation\n* 2.3. The Machine Learning\n\n####  3. Analyzing the Results\n* 3.1. Analyzing Seasonalities\n* 3.2. Overlapping of Actual Values with Expected Values\n* 3.3. Trained Model Evaluation\n\n#### 4. Exporting Data\n* 4.1. Exporting Data to IBM Cloud Object Storage"}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "# 1: Mining Stock Market Data"}, {"metadata": {}, "cell_type": "markdown", "source": "The data mining step is done using the Quandl API, a financial database (with both public and private tables) purchased by NASDAQ in 2018. It is not mandatory to use an API key to extract data, however it is recommended to generate one for free <a href=\"https://www.quandl.com/\">at the Quandl website</a>."}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.1: Quandl API Setup"}, {"metadata": {}, "cell_type": "code", "source": "# Install quandl package (this can take some minutes)\n!pip install --user quandl==3.4.8 --upgrade", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Requirement already up-to-date: quandl==3.4.8 in /home/dsxuser/.local/lib/python3.6/site-packages (3.4.8)\nRequirement already satisfied, skipping upgrade: pyOpenSSL in /opt/conda/envs/Python36/lib/python3.6/site-packages (from quandl==3.4.8) (19.0.0)\nRequirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/envs/Python36/lib/python3.6/site-packages (from quandl==3.4.8) (2.7.5)\nRequirement already satisfied, skipping upgrade: more-itertools<=5.0.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from quandl==3.4.8) (5.0.0)\nRequirement already satisfied, skipping upgrade: requests>=2.7.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from quandl==3.4.8) (2.21.0)\nRequirement already satisfied, skipping upgrade: ndg-httpsclient in /home/dsxuser/.local/lib/python3.6/site-packages (from quandl==3.4.8) (0.5.1)\nRequirement already satisfied, skipping upgrade: pandas>=0.14 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from quandl==3.4.8) (0.24.1)\nRequirement already satisfied, skipping upgrade: six in /opt/conda/envs/Python36/lib/python3.6/site-packages (from quandl==3.4.8) (1.12.0)\nRequirement already satisfied, skipping upgrade: inflection>=0.3.1 in /home/dsxuser/.local/lib/python3.6/site-packages (from quandl==3.4.8) (0.5.0)\nRequirement already satisfied, skipping upgrade: numpy>=1.8 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from quandl==3.4.8) (1.15.4)\nRequirement already satisfied, skipping upgrade: pyasn1 in /home/dsxuser/.local/lib/python3.6/site-packages (from quandl==3.4.8) (0.4.8)\nRequirement already satisfied, skipping upgrade: cryptography>=2.3 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pyOpenSSL->quandl==3.4.8) (2.5)\nRequirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests>=2.7.0->quandl==3.4.8) (2.8)\nRequirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests>=2.7.0->quandl==3.4.8) (1.24.1)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests>=2.7.0->quandl==3.4.8) (2020.4.5.2)\nRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from requests>=2.7.0->quandl==3.4.8) (3.0.4)\nRequirement already satisfied, skipping upgrade: pytz>=2011k in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pandas>=0.14->quandl==3.4.8) (2018.9)\nRequirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from cryptography>=2.3->pyOpenSSL->quandl==3.4.8) (1.11.5)\nRequirement already satisfied, skipping upgrade: asn1crypto>=0.21.0 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from cryptography>=2.3->pyOpenSSL->quandl==3.4.8) (0.24.0)\nRequirement already satisfied, skipping upgrade: pycparser in /opt/conda/envs/Python36/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3->pyOpenSSL->quandl==3.4.8) (2.19)\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Import quandl package\nimport quandl\nprint(\"Quandl package imported.\")", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "Quandl package imported.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "If you registered and generated an API key for free on the Quandl web page, type it in the variable in the cell below."}, {"metadata": {}, "cell_type": "code", "source": "quandl.ApiConfig.api_key = \"p71xJ-ssjc2HWQn1ZXw4\"", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.2: Downloading the WIKI/PRICES Table"}, {"metadata": {}, "cell_type": "markdown", "source": "Quandl has several public and private data tables available.\n\nIn this project we will use the table <a href=\"https://www.quandl.com/databases/WIKIP\"> WIKI/PRICES</a>. This table has information on the daily closing and opening values of the shares of more than 3000 companies.\n\nThe data download is done via an API call executed with the quandl library, using the `get_table` function. The arguments of this function are: the table name, the <a href=\"https://en.wikipedia.org/wiki/Ticker_symbol\"> ticker symbol </a> of the desired stock, and the desired columns of the table. The `paginate=True` parameter allows more than 10000 rows of data to be transferred with a single call.\n\nThe result of this function call is a pandas dataframe with the desired stock data, ready to be transformed."}, {"metadata": {}, "cell_type": "code", "source": "# API Call example - Downloading financial data from IBM stocks\ndata = quandl.get_table('WIKI/PRICES',\n                        ticker='IBM', \n                        qopts={'columns':['date', 'open', 'high', 'low', 'close']},\n                        paginate=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "At the <a href=\"https://www.nasdaq.com/symbol/\">NASDAQ page</a> you can see the ticker symbols for various stocks.\n\nYou can also choose a ticker from the ones listed in the `nasdaq-ticker-table.csv` file, which can be accessed <a href =\"https://github.com/vanderleipf/ibmdegla-ws-projects/blob/master/EN-US/forecasting-the-stock-market/data/stock-ticker-table.csv\">here</a>.\n\nChoose the ticker of another company, or use the data already configured and saved in the dataframe `date` by the API call executed in the above code cell."}, {"metadata": {}, "cell_type": "code", "source": "# Renaming the pandas dataframe columns\ndata.columns = ['Date', 'Open', 'High', 'Low', 'Close']\n\n# Reset the pandas dataframe index\ndata = data.reset_index(drop=True)\n\n# Show the last five rows of the pandas dataframe\ndata.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.3: Visualizing the Collected Data"}, {"metadata": {}, "cell_type": "markdown", "source": "To view the data, the `bokeh` package is used. This module is capable of generating interactive Javascript charts."}, {"metadata": {}, "cell_type": "code", "source": "!pip install --user bokeh==1.0.4 --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from bokeh.plotting import figure, output_file, show\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.embed import components\nfrom bokeh.io import output_notebook\n\nprint('Packages imported.')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Load bokeh\noutput_notebook()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Bokeh Figure\np = figure(plot_width=1200, plot_height=550, title='Stock Value Historical Data', x_axis_type=\"datetime\")\n\n# Plot Lines\np.line(data.Date, data.Open, line_width=2, line_color=\"#0099ff\", legend='Open')\np.line(data.Date, data.Close, line_width=2, line_color=\"#ff6699\", legend='Close')\np.line(data.Date, data.High, line_width=1, line_color=\"#000000\", legend='High')\np.line(data.Date, data.Low, line_width=1, line_color=\"#000000\", legend='Low')\n\n# Axis and Labels\np.legend.orientation = \"vertical\"\np.xaxis.axis_label = \"Date\"\np.xaxis.axis_label_text_font_style = 'bold'\np.xaxis.axis_label_text_font_size = '16pt'\np.xaxis.major_label_text_font_size = '14pt'\np.yaxis.axis_label = \"Value ($ USD)\"\np.yaxis.axis_label_text_font_style = 'bold'\np.yaxis.axis_label_text_font_size = '16pt'\np.yaxis.major_label_text_font_size = '12pt'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "In the figure below we can see and interact with the data extracted from the WIKI/PRICES table. In blue we have the opening value of the stock; in red the closing value of the stock; in black we have the maximum and minimum values (daily)."}, {"metadata": {}, "cell_type": "code", "source": "show(p)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "# 2: Training a Predictive Model using Open-Source Libraries"}, {"metadata": {}, "cell_type": "markdown", "source": "In the training process of our predictive model, we will use the `fbprophet` package, developed by Facebook for time series analysis.\n\nFbprophet follows the same style of objects as `sklearn`, an extremely popular machine learning python library. An instance of the Prophet class is created and then the `fit` and` predict` methods are used.\n\nThe training set of a Prophet model is a two-column pandas dataframe with the columns `ds` and` y`. The `ds` (datestamp) column must be a date in the format YYYY-MM-DD, or a timestamp in the format YYYY-MM-DD HH:MM:SS. The `y` column must be numeric, and represents the variable we wish to model in the future."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1: Fbprophet Setup"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# Install required packages (this can take some minutes)\n# UPDATE(vnderlev): setuptools-git is now required for fbprophet 0.5. \n!pip install --user pystan==2.17.1.0 holidays==0.9.8 setuptools-git==1.2 matplotlib==3.0.2 --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Install fbprophet from conda-forge (this will take some time, please be patient)\n!conda install -c conda-forge fbprophet=0.5 -y", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Import packages\nimport fbprophet\nimport datetime as dt\n\nprint('Packages imported.')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.2: Data Preparation"}, {"metadata": {}, "cell_type": "markdown", "source": "First of all, the previously collected dataframe needs to be prepared.\n\nThe preparation includes the selection of a subset of the data for model training. In this Jupyter notebook we will only sample the latest data, starting from 2008.\n\nOf the four columns with stock values, only one column will be chosen as the variable of interest. We will choose the daily opening value of the shares (the `Open` column)."}, {"metadata": {}, "cell_type": "code", "source": "# Select train sample\ndf_train = data.copy()\ndf_train = df_train[(df_train.Date > dt.datetime(2008,1,1))]\ndf_train.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Format the dataframe for FBProphet\ndf_train.rename(columns={'Open':'y', 'Date':'ds'}, inplace=True)\ndf_train = df_train.filter(items=['ds', 'y'])\ndf_train.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.3: The Machine Learning"}, {"metadata": {}, "cell_type": "markdown", "source": "As previously mentioned, Prophet uses the same interface style as the sklearn library.\n\nA Prophet class is instantiated, with the desired types of seasonality. For this work, we will consider annual and monthly seasonalities in stock values, and disregard daily and weekly effects.\n\nUsually the presented settings are the most adequate for stock prediction, but feel free to test different seasonalities."}, {"metadata": {}, "cell_type": "code", "source": "# Instantiate a fbprophet model\nmodel = fbprophet.Prophet(daily_seasonality=False,\n                weekly_seasonality=False, \n                yearly_seasonality=True,\n                changepoint_prior_scale=0.05,\n                changepoints=None)\nmodel.add_seasonality(name='monthly', period=30.5, fourier_order=5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Train model\nmodel.fit(df_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "In the following cell, the prediction runs in a period of 365 days (1 year) in the future. The result is a multi-column dataframe with information about trends, tolerances, and forecasts."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# Execute forecasting algorithm (1 year into the future)\nfuture_data = model.make_future_dataframe(periods=365, freq='D')\nfuture_data = model.predict(future_data)\nfuture_data.tail()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "## 3: Results"}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1: Analyzing Seasonalities"}, {"metadata": {}, "cell_type": "markdown", "source": "]In the figure below we have a quick plot of the results using the Prophet class `plot` method.\n\nThe black dots are the actual data, the dark blue line is the modeled trend, and in light blue we have the tolerance (minimum and maximum values modeled)."}, {"metadata": {}, "cell_type": "code", "source": "fig1 = model.plot(future_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Using the `plot_components` function of the Prophet object, we can also analyze trend information, as well as the monthly and yearly seasonalities considered in the modeling."}, {"metadata": {}, "cell_type": "code", "source": "fig2 = model.plot_components(future_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2: Overlapping of Actual Values with Expected Values"}, {"metadata": {}, "cell_type": "code", "source": "# Bokeh Figure\np = figure(plot_width=1200, plot_height=550, title='Stock Value Historical Data', x_axis_type=\"datetime\")\n\n# Plot Lines\np.line(data[data['Date'] > dt.datetime(2008,1,1)].Date, data[data['Date'] > dt.datetime(2008,1,1)].Open, line_width=2, line_color=\"#0099ff\", legend='Observed Open Value')\np.line(future_data.ds, future_data.yhat, line_width=2, line_color=\"#2B0000\", legend='Modeled Open Value')\np.line(future_data.ds, future_data.yhat_upper, line_width=0.5, line_color=\"#000099\", legend='Upper Estimates')\np.line(future_data.ds, future_data.yhat_lower, line_width=0.5, line_color=\"#000099\", legend='Lower Estimates')\n\n# Axis and Labels\np.legend.orientation = \"vertical\"\np.xaxis.axis_label = \"Date\"\np.xaxis.axis_label_text_font_style = 'bold'\np.xaxis.axis_label_text_font_size = '16pt'\np.xaxis.major_label_text_font_size = '14pt'\np.yaxis.axis_label = \"Value ($ USD)\"\np.yaxis.axis_label_text_font_style = 'bold'\np.yaxis.axis_label_text_font_size = '16pt'\np.yaxis.major_label_text_font_size = '12pt'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "In the next cell we can thoroughly analyze the quality of the trained model by comparing the modeled values (in black) with the actual observed values (light blue)."}, {"metadata": {}, "cell_type": "code", "source": "show(p)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.3: Trained Model Evaluation"}, {"metadata": {}, "cell_type": "markdown", "source": "The Prophet class also provides the means to perform an evaluation of the created model.\n\nThe `cross-validation` method will be used to evaluate our model."}, {"metadata": {}, "cell_type": "code", "source": "from fbprophet.diagnostics import cross_validation\ndf_cv = cross_validation(model, initial='730 days', period='180 days', horizon='365 days')\ndf_cv.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from fbprophet.diagnostics import performance_metrics\ndf_p = performance_metrics(df_cv)\ndf_p.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from fbprophet.plot import plot_cross_validation_metric\nfig = plot_cross_validation_metric(df_cv, metric='mape')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Analyzing the `MAPE` criterion projected in the cell above, it is noticed that the error for a forecast of up to 50 days turns around 5%, reaching 15% for a forecast of 365 days. This error is a relatively good metric if the objective is to construct an image of the market trends. For day-trading purposes, even 95% accuracy is not really adequate."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "# 4: Exporting Data"}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1: Exporting Data to IBM Cloud Object Storage"}, {"metadata": {}, "cell_type": "markdown", "source": "The mined data, as well as the data produced by the model, can be exported as CSV files to IBM Cloud Object Storage, and eventually be used or published in other applications.\n\nNext we present the code necessary to perform this task in an automated way."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_botocore.client import Config\nimport ibm_boto3, os\n\nprint('Packages imported.')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Set up your IBM Cloud Object Storage credentials in the next cell."}, {"metadata": {}, "cell_type": "code", "source": "\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncos_credentials = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-6804be72-5799-4744-85df-393abcbc6918',\n    'IBM_API_KEY_ID': '3EkUP2gzWymRvZ_NnbFkQMVNbLNvAyvDGT8395q8vCae',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n    'BUCKET': 'trialproject2-donotdelete-pr-c70j6vphqntwsy',\n    'FILE': 'AAPL.csv'\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The `upload_data_to_ibm_cos` is a function created to upload pandas dataframes as CSV files in IBM COS."}, {"metadata": {}, "cell_type": "code", "source": "def upload_data_to_ibm_cos(credentials, df, df_future, tick):\n    cos = ibm_boto3.client(service_name='s3', \n                           ibm_api_key_id=credentials['IBM_API_KEY_ID'], \n                           ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n                           ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n                           config=Config(signature_version='oauth'),\n                           endpoint_url=credentials['ENDPOINT'])\n    df.to_csv('{}.csv'.format(tick), sep=',', encoding='utf-8')\n    df_future.to_csv('{}_future.csv'.format(tick), sep=',', encoding='utf-8')\n    try:\n        res=cos.upload_file(Filename='{}.csv'.format(tick), Bucket=credentials['BUCKET'], Key='{}.csv'.format(tick))\n        res=cos.upload_file(Filename='{}_future.csv'.format(tick), Bucket=credentials['BUCKET'], Key='{}_future.csv'.format(tick))\n    except Exception as e:\n        print(Exception, e)\n    else:\n        print(\"{} data uploaded to IBM COS.\".format(tick))\n        os.remove('{}.csv'.format(tick))\n        os.remove('{}_future.csv'.format(tick))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Upload mined and modeled data to IBM COS\nupload_data_to_ibm_cos(cos_credentials, data, future_data, 'IBM')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can now access the `<stock_ticker>.csv` and `<stock_ticker>_future.csv` file anywhere using the IBM COS API."}, {"metadata": {}, "cell_type": "markdown", "source": "<hr>\n\n\n## Want to Learn More?\n\nWatch the public video on how to replicate this analysis with SPSS Modeler, an analytics platform that has several embedded machine learning algorithms. This platform is designed to facilitate data analysis through a graphical programming language, and is integrated with Watson Studio.\n\nThis notebook and its source code is made available under the terms of the <a href = \"https://github.com/IBM/watson-stock-market-predictor/blob/master/LICENSE\">Apache License 2.0</a>.\n\n<hr>"}, {"metadata": {}, "cell_type": "markdown", "source": "### Thank you for completing this journey!"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, 
"pygments_lexer": "ipython3", 
"nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}
data = quandl.get_table('WIKI/PRICES',
                        ticker='IBM', 
                        qopts={'columns':['date', 'open', 'high', 'low', 'close']},
                        paginate=True)
                        from bokeh.models import ColumnDataSource
from bokeh.embed import components
from bokeh.io import output_notebook

print('Packages imported.')
# Bokeh Figure
p = figure(plot_width=1200, plot_height=550, title='Stock Value Historical Data', x_axis_type="datetime")

# Plot Lines
p.line(data.Date, data.Open, line_width=2, line_color="#0099ff", legend='Open')
p.line(data.Date, data.Close, line_width=2, line_color="#ff6699", legend='Close')
p.line(data.Date, data.High, line_width=1, line_color="#000000", legend='High')
p.line(data.Date, data.Low, line_width=1, line_color="#000000", legend='Low')

# Axis and Labels
p.legend.orientation = "vertical"
p.xaxis.axis_label = "Date"
p.xaxis.axis_label_text_font_style = 'bold'
p.xaxis.axis_label_text_font_size = '16pt'
p.xaxis.major_label_text_font_size = '14pt'
p.yaxis.axis_label = "Value ($ USD)"
p.yaxis.axis_label_text_font_style = 'bold'
p.yaxis.axis_label_text_font_size = '16pt'
p.yaxis.major_label_text_font_size = '12pt'
# Install required packages (this can take some minutes)
# UPDATE(vnderlev): setuptools-git is now required for fbprophet 0.5. 
!pip install --user pystan==2.17.1.0 holidays==0.9.8 setuptools-git==1.2 matplotlib==3.0.2 --upgrade
# Install fbprophet from conda-forge (this will take some time, please be patient)
!conda install -c conda-forge fbprophet=0.5 -y
# Import packages
import fbprophet
import datetime as dt

print('Packages imported.')

df_train = data.copy()
df_train = df_train[(df_train.Date > dt.datetime(2008,1,1))]
df_train.tail()
# Format the dataframe for FBProphet
df_train.rename(columns={'Open':'y', 'Date':'ds'}, inplace=True)
df_train = df_train.filter(items=['ds', 'y'])
df_train.tail()

# Instantiate a fbprophet model
model = fbprophet.Prophet(daily_seasonality=False,
                weekly_seasonality=False, 
                yearly_seasonality=True,
                changepoint_prior_scale=0.05,
                changepoints=None)
model.add_seasonality(name='monthly', period=30.5, fourier_order=5)
# Train model
model.fit(df_train)

future_data = model.make_future_dataframe(periods=365, freq='D')
future_data = model.predict(future_data)
future_data.tail()

# Bokeh Figure
p = figure(plot_width=1200, plot_height=550, title='Stock Value Historical Data', x_axis_type="datetime")

# Plot Lines
p.line(data[data['Date'] > dt.datetime(2008,1,1)].Date, data[data['Date'] > dt.datetime(2008,1,1)].Open, line_width=2, line_color="#0099ff", legend='Observed Open Value')
p.line(future_data.ds, future_data.yhat, line_width=2, line_color="#2B0000", legend='Modeled Open Value')
p.line(future_data.ds, future_data.yhat_upper, line_width=0.5, line_color="#000099", legend='Upper Estimates')
p.line(future_data.ds, future_data.yhat_lower, line_width=0.5, line_color="#000099", legend='Lower Estimates')

# Axis and Labels
p.legend.orientation = "vertical"
p.xaxis.axis_label = "Date"
p.xaxis.axis_label_text_font_style = 'bold'
p.xaxis.axis_label_text_font_size = '16pt'
p.xaxis.major_label_text_font_size = '14pt'
p.yaxis.axis_label = "Value ($ USD)"
p.yaxis.axis_label_text_font_style = 'bold'
p.yaxis.axis_label_text_font_size = '16pt'
p.yaxis.major_label_text_font_size = '12pt'

from fbprophet.diagnostics import cross_validation
df_cv = cross_validation(model, initial='730 days', period='180 days', horizon='365 days')
df_cv.head()
from fbprophet.diagnostics import performance_metrics
df_p = performance_metrics(df_cv)
df_p.head()
from fbprophet.plot import plot_cross_validation_metric
fig = plot_cross_validation_metric(df_cv, metric='mape')

from ibm_botocore.client import Config
import ibm_boto3, os

print('Packages imported.')

def upload_data_to_ibm_cos(credentials, df, df_future, tick):
    cos = ibm_boto3.client(service_name='s3', 
                           ibm_api_key_id=credentials['IBM_API_KEY_ID'], 
                           ibm_service_instance_id=credentials['IAM_SERVICE_ID'],
                           ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],
                           config=Config(signature_version='oauth'),
                           endpoint_url=credentials['ENDPOINT'])
    df.to_csv('{}.csv'.format(tick), sep=',', encoding='utf-8')
    df_future.to_csv('{}_future.csv'.format(tick), sep=',', encoding='utf-8')
    try:
        res=cos.upload_file(Filename='{}.csv'.format(tick), Bucket=credentials['BUCKET'], Key='{}.csv'.format(tick))
        res=cos.upload_file(Filename='{}_future.csv'.format(tick), Bucket=credentials['BUCKET'], Key='{}_future.csv'.format(tick))
    except Exception as e:
        print(Exception, e)
    else:
        print("{} data uploaded to IBM COS.".format(tick))
        os.remove('{}.csv'.format(tick))
        os.remove('{}_future.csv'.format(tick))
        
        # Upload mined and modeled data to IBM COS
upload_data_to_ibm_cos(cos_credentials, data, future_data, 'IBM')
